{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Group 5 - ED(B) - Deep Learning Approach without the use of Transformers - Demo"
      ],
      "metadata": {
        "id": "FSPZiKbwpLrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Management"
      ],
      "metadata": {
        "id": "v9JJywR7qNJf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5glhMS3o_V-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for NLTK data\n",
        "nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "\n",
        "# Set the NLTK data path\n",
        "nltk.data.path.append(nltk_data_dir)\n",
        "\n",
        "# Download punkt to the specified directory\n",
        "nltk.download('punkt_tab', download_dir=nltk_data_dir)"
      ],
      "metadata": {
        "id": "LwJhLGgkYSNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create output directory for models and plots\n",
        "os.makedirs('models', exist_ok=True)"
      ],
      "metadata": {
        "id": "AR_5P1dUqX-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evidence_detection.bilstm_with_attention import BiLSTMAttention\n",
        "from evidence_detection.vocabulary import Vocabulary\n",
        "from evidence_detection.evidence_detection_dataset import EvidenceDetectionDataset"
      ],
      "metadata": {
        "id": "N3LTpB8A-ESd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load test set"
      ],
      "metadata": {
        "id": "2fP5so_aZzX-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO6PRKnBZzX_"
      },
      "outputs": [],
      "source": [
        "test_path = 'data/test.csv'\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the vocabulary and set the embedding dimension\n",
        "\n",
        "The vocabulary \"vocab.pkl\" an be downloaded from the following link: https://drive.google.com/drive/folders/1TWv5UKNsNeQGxafx3GQf87Dc8vcp5V8t"
      ],
      "metadata": {
        "id": "4Ddb9kXBQu6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vocabulary\n",
        "with open('vocab.pkl', 'rb') as f:\n",
        "    vocab = pickle.load(f)"
      ],
      "metadata": {
        "id": "xmQ5wmJS292k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same embedding dimension used in training\n",
        "embedding_dim = 300"
      ],
      "metadata": {
        "id": "IjTPin1qX8m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create test dataset and dataloader"
      ],
      "metadata": {
        "id": "Xdptq83IScgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  \"\"\"\n",
        "  Custom collate function to handle variable-length sequences and\n",
        "  to collate batches without labels for test data.\n",
        "\n",
        "  Args:\n",
        "    batch: A batch of data from the dataset\n",
        "\n",
        "  Returns:\n",
        "    Dictionary with padded sequences and other batch information\n",
        "  \"\"\"\n",
        "  # Separate batch elements\n",
        "  claims = [item['claim_ids'] for item in batch]\n",
        "  claim_lengths = torch.tensor([item['claim_length'] for item in batch])\n",
        "  evidences = [item['evidence_ids'] for item in batch]\n",
        "  evidence_lengths = torch.tensor([item['evidence_length'] for item in batch])\n",
        "\n",
        "  # Pad sequences\n",
        "  padded_claims = pad_sequence(claims, batch_first=True, padding_value=0)\n",
        "  padded_evidences = pad_sequence(evidences, batch_first=True, padding_value=0)\n",
        "\n",
        "  return {\n",
        "    'claim_ids': padded_claims,\n",
        "    'claim_lengths': claim_lengths,\n",
        "    'evidence_ids': padded_evidences,\n",
        "    'evidence_lengths': evidence_lengths,\n",
        "  }"
      ],
      "metadata": {
        "id": "C-aNGSEp48sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "test_dataset = EvidenceDetectionDataset(test_df, vocab, is_test=True)\n",
        "print(f\"Test dataset created: {len(test_dataset)} samples\")"
      ],
      "metadata": {
        "id": "MqPwNaH73Yb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe5334a6-c998-4c47-9991-d4810b02c098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset created: 5926 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test data loader for testing\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "NWIofl_E5Ntm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model architecture and weights\n",
        "\n",
        "The model weights \"ED_model_B.pt\" and architecture \"model_architecture_parameters/parameters.pkl\" can be downloaded from the following link:\n",
        "https://drive.google.com/drive/folders/1TWv5UKNsNeQGxafx3GQf87Dc8vcp5V8t"
      ],
      "metadata": {
        "id": "QGiSImv2wyTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"ED_model_B.pt\"\n",
        "\n",
        "# Load the best hyperparameters\n",
        "with open('model_architecture_parameters/parameters.pkl', 'rb') as f:\n",
        "    best_params = pickle.load(f)\n",
        "\n",
        "print(\"Loaded hyperparameters:\", best_params)\n",
        "model = BiLSTMAttention(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    num_layers=1,\n",
        "    hidden_dim=best_params['hidden_dim'],\n",
        "    dropout=best_params['dropout'],\n",
        ")\n",
        "\n",
        "# Load the trained weights\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "print(\"\\nModel loaded successfully.\\n\")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "k83WRdv3vnUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model"
      ],
      "metadata": {
        "id": "IHG7ZMdPcskD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Run inference\n",
        "print(\"\\nMaking predictions on test data...\\n\")\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        # Move batch to device\n",
        "        claim_ids = batch['claim_ids'].to(device)\n",
        "        claim_lengths = batch['claim_lengths']\n",
        "        evidence_ids = batch['evidence_ids'].to(device)\n",
        "        evidence_lengths = batch['evidence_lengths']\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(claim_ids, claim_lengths, evidence_ids, evidence_lengths)\n",
        "\n",
        "        # Get predictions (0 or 1)\n",
        "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        all_predictions.extend(predictions)\n",
        "\n",
        "# Save predictions to CSV\n",
        "predictions_df = pd.DataFrame({'prediction': all_predictions})\n",
        "predictions_df.to_csv('Group_5_B.csv', index=False)\n",
        "print(f\"Predictions saved to 'Group_5_B.csv'\")"
      ],
      "metadata": {
        "id": "t4J9rWMPtFTz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}