{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evidence Detection\n",
        "\n",
        "## Data is in the form of: Claim, Evidence, Labels\n",
        "\n",
        "## Labels\n",
        "- **1 (Relevant)** - The evidence supports or is related to the claim.\n",
        "- **0 (Not Relevant)** â€“ The evidence does not support or is unrelated to the claim."
      ],
      "metadata": {
        "id": "IiaRhQWzOf7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Management"
      ],
      "metadata": {
        "id": "3eMIU_UuOEam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Create a directory for NLTK data\n",
        "nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "\n",
        "# Set the NLTK data path\n",
        "nltk.data.path.append(nltk_data_dir)\n",
        "\n",
        "# Download punkt to the specified directory\n",
        "nltk.download('punkt_tab', download_dir=nltk_data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ishUV1aWNML",
        "outputId": "5d98b1a5-1c60-4fe3-d0b7-e74416bcc045"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evidence_detection.bilstm_with_attention import BiLSTMAttention\n",
        "from evidence_detection.evidence_detection_dataset import EvidenceDetectionDataset\n",
        "from evidence_detection.vocabulary import Vocabulary\n",
        "from evidence_detection.trainer import Trainer"
      ],
      "metadata": {
        "id": "Ju4U0HGxY6TW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create output directory for models and plots\n",
        "os.makedirs('models', exist_ok=True)\n",
        "# os.makedirs('plots', exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq1MSjuFMj6d",
        "outputId": "2ded9f72-9c2f-4837-d969-4a39a7ab5efb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Glove Embedding Method"
      ],
      "metadata": {
        "id": "vrG-YMS_NcI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(vocab, glove_path, embedding_dim=300):\n",
        "    \"\"\"Load GloVe embeddings for words in vocabulary\"\"\"\n",
        "    embeddings = np.zeros((len(vocab), embedding_dim))\n",
        "\n",
        "    # Initialize random embeddings\n",
        "    for i in range(len(vocab)):\n",
        "        embeddings[i] = np.random.normal(scale=0.1, size=(embedding_dim, ))\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    if not os.path.exists(glove_path):\n",
        "        print(f\"GloVe embeddings not found at {glove_path}. Using random embeddings.\")\n",
        "        return embeddings\n",
        "\n",
        "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
        "\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(tqdm(f, desc=\"Loading GloVe\")):\n",
        "            try:\n",
        "                values = line.split()\n",
        "\n",
        "                # Check if the vector has the correct dimension\n",
        "                if len(values) != embedding_dim + 1:  # +1 for the word itself\n",
        "                    print(f\"Warning: Line {i} has {len(values)} values, expected {embedding_dim + 1}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                word = values[0]\n",
        "                if word in vocab.stoi:\n",
        "                    vector = np.array(values[1:], dtype='float32')\n",
        "\n",
        "                    # Double-check vector dimension\n",
        "                    if len(vector) != embedding_dim:\n",
        "                        print(f\"Warning: Vector for word '{word}' has dimension {len(vector)}, expected {embedding_dim}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    embeddings[vocab.stoi[word]] = vector\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Loaded {embedding_dim}-dimensional GloVe embeddings.\")\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "LKKKt7fcWhJy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Data, build vocabulary and Build Glove Embeddings"
      ],
      "metadata": {
        "id": "R7qYm1XMSUer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "# train_df = pd.read_csv('./data/train.csv')\n",
        "# val_df = pd.read_csv('./data/dev.csv')\n",
        "print(\"Using train.csv for train and validation\")\n",
        "print(\"Using dev.csv for testing\")\n",
        "train_df = pd.read_csv('./data/train.csv')\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "test_df = pd.read_csv('./data/dev.csv')\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}, Validation shape: {val_df.shape}\")\n",
        "print(f\"Label distribution in train: {train_df['label'].value_counts().to_dict()}\")\n",
        "print(f\"Label distribution in val: {val_df['label'].value_counts().to_dict()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJg4xvC2SKR7",
        "outputId": "9bc46109-2136-40bd-a597-c5d286b6c425"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "Using train.csv for train and validation\n",
            "Using dev.csv for testing\n",
            "Train shape: (17206, 3), Validation shape: (4302, 3)\n",
            "Label distribution in train: {0: 12504, 1: 4702}\n",
            "Label distribution in val: {0: 3150, 1: 1152}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load or create vocabulary\n",
        "vocab_path = 'vocab.pkl'\n",
        "if os.path.exists(vocab_path):\n",
        "    print(f\"Loading vocabulary from {vocab_path}\")\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "else:\n",
        "    print(\"Creating new vocabulary\")\n",
        "    vocab = Vocabulary(freq_threshold=3)\n",
        "    all_texts = train_df['Claim'].tolist() + train_df['Evidence'].tolist()\n",
        "    vocab.build_vocabulary(all_texts)\n",
        "\n",
        "    # Save vocabulary\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump(vocab, f)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zHL9dr3SPOq",
        "outputId": "be823c9f-1b8f-48a5-b332-90aaf74d3d59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new vocabulary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34412/34412 [00:04<00:00, 8333.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 12686\n",
            "Vocabulary size: 12686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings\n",
        "embedding_dim = 300\n",
        "glove_path = \"glove.6B.300d.txt\"\n",
        "embeddings = load_glove_embeddings(vocab, glove_path, embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5yEURMYSRzc",
        "outputId": "1163f0f6-f461-44dd-edea-98c4ef8735a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings from glove.6B.300d.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading GloVe: 200582it [00:03, 50552.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Line 200581 has 94 values, expected 301. Skipping.\n",
            "Loaded 300-dimensional GloVe embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model"
      ],
      "metadata": {
        "id": "hARI31lKTTKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = EvidenceDetectionDataset(train_df, vocab)\n",
        "val_dataset = EvidenceDetectionDataset(val_df, vocab)\n",
        "test_dataset = EvidenceDetectionDataset(test_df, vocab)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Model hyperparameters\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "dropout = 0.4\n",
        "\n",
        "# Initialize model\n",
        "model = BiLSTMAttention(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout,\n",
        "    pretrained_embeddings=embeddings\n",
        ")\n",
        "\n",
        "# Print model architecture summary\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")\n",
        "print(f\"Embedding Dimension: {embedding_dim}\")\n",
        "print(f\"Hidden Dimension: {hidden_dim}\")\n",
        "print(f\"Number of LSTM Layers: {num_layers}\")\n",
        "print(f\"Dropout Rate: {dropout}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-5\n",
        "num_epochs = 15\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"\\nStarting training...\")\n",
        "trainer.train(num_epochs=num_epochs)\n",
        "\n",
        "# Final evaluation\n",
        "print(\"\\nPerforming final evaluation on validation set...\")\n",
        "val_metrics = trainer.evaluate()\n",
        "\n",
        "print(\"\\nFinal Validation Metrics:\")\n",
        "print(f\"Loss: {val_metrics['loss']:.4f}\")\n",
        "print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
        "print(f\"F1 Score: {val_metrics['f1']:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = 'models/final_model.pt'\n",
        "trainer.save_model(final_model_path)\n",
        "\n",
        "print(\"\\nTraining and evaluation completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OBaBa6JxJ8Z",
        "outputId": "0d5a1c86-248d-4de2-8bf6-fc47a6252abe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 17206\n",
            "Validation dataset size: 4302\n",
            "Test dataset size: 5926\n",
            "\n",
            "Model Architecture:\n",
            "Vocabulary Size: 12686\n",
            "Embedding Dimension: 300\n",
            "Hidden Dimension: 256\n",
            "Number of LSTM Layers: 1\n",
            "Dropout Rate: 0.4\n",
            "Total Parameters: 4,884,332\n",
            "Trainable Parameters: 4,884,332\n",
            "\n",
            "Starting training...\n",
            "Starting training on device: cuda\n",
            "Training set size: 17206\n",
            "Validation set size: 4302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 538/538 [00:16<00:00, 31.95it/s]\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:01<00:00, 68.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - Time: 18.88s\n",
            "Train Loss: 0.4447, Train F1: 0.5409\n",
            "Val Loss: 0.3782, Val F1: 0.6112\n",
            "Val Precision: 0.7617, Val Recall: 0.5104\n",
            "Validation F1 increased (0.000000 --> 0.611227). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 538/538 [00:15<00:00, 35.48it/s]\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:01<00:00, 68.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15 - Time: 17.19s\n",
            "Train Loss: 0.3416, Train F1: 0.7053\n",
            "Val Loss: 0.3596, Val F1: 0.6746\n",
            "Val Precision: 0.7135, Val Recall: 0.6398\n",
            "Validation F1 increased (0.611227 --> 0.674600). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 538/538 [00:15<00:00, 35.22it/s]\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:01<00:00, 69.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15 - Time: 17.29s\n",
            "Train Loss: 0.2478, Train F1: 0.8011\n",
            "Val Loss: 0.4162, Val F1: 0.6708\n",
            "Val Precision: 0.6465, Val Recall: 0.6970\n",
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 538/538 [00:15<00:00, 35.63it/s]\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:01<00:00, 71.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15 - Time: 17.06s\n",
            "Train Loss: 0.1505, Train F1: 0.8925\n",
            "Val Loss: 0.5544, Val F1: 0.6344\n",
            "Val Precision: 0.6689, Val Recall: 0.6033\n",
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 538/538 [00:14<00:00, 35.89it/s]\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:01<00:00, 68.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15 - Time: 17.04s\n",
            "Train Loss: 0.0785, Train F1: 0.9501\n",
            "Val Loss: 0.7167, Val F1: 0.6092\n",
            "Val Precision: 0.6417, Val Recall: 0.5799\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping triggered\n",
            "Loaded best model from 'best_bilstm_attention_model.pt'\n",
            "\n",
            "Performing final evaluation on validation set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [00:01<00:00, 69.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Validation Metrics:\n",
            "Loss: 0.3596\n",
            "Accuracy: 0.8347\n",
            "Precision: 0.7135\n",
            "Recall: 0.6398\n",
            "F1 Score: 0.6746\n",
            "Model saved to models/final_model.pt\n",
            "\n",
            "Training and evaluation completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model"
      ],
      "metadata": {
        "id": "IHG7ZMdPcskD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test data loader for final testing\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=trainer.collate_fn\n",
        ")\n",
        "\n",
        "# Now evaluate on test set\n",
        "print(\"\\nPerforming evaluation on test set (dev.csv)...\")\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        # Move batch to device\n",
        "        claim_ids = batch['claim_ids'].to(device)\n",
        "        claim_lengths = batch['claim_lengths']\n",
        "        evidence_ids = batch['evidence_ids'].to(device)\n",
        "        evidence_lengths = batch['evidence_lengths']\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits, _ = model(claim_ids, claim_lengths, evidence_ids, evidence_lengths)\n",
        "        loss = trainer.criterion(logits, labels)\n",
        "\n",
        "        # Accumulate metrics\n",
        "        test_loss += loss.item()\n",
        "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        all_predictions.extend(predictions)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "precision = precision_score(all_labels, all_predictions, average='binary')\n",
        "recall = recall_score(all_labels, all_predictions, average='binary')\n",
        "f1 = f1_score(all_labels, all_predictions, average='binary')\n",
        "\n",
        "print(\"\\nTest Set Metrics:\")\n",
        "print(f\"Loss: {test_loss / len(test_loader):.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7agV0aF7a_3Z",
        "outputId": "dd9d2f83-116d-43eb-df53-731beb9da76e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing evaluation on test set (dev.csv)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 186/186 [00:02<00:00, 68.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Metrics:\n",
            "Loss: 0.3728\n",
            "Accuracy: 0.8237\n",
            "Precision: 0.7105\n",
            "Recall: 0.6122\n",
            "F1 Score: 0.6577\n"
          ]
        }
      ]
    }
  ]
}