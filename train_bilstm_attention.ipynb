{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Create a directory for NLTK data\n",
        "nltk_data_dir = os.path.join(os.getcwd(), 'nltk_data')\n",
        "os.makedirs(nltk_data_dir, exist_ok=True)\n",
        "\n",
        "# Set the NLTK data path\n",
        "nltk.data.path.append(nltk_data_dir)\n",
        "\n",
        "# Download punkt to the specified directory\n",
        "nltk.download('punkt_tab', download_dir=nltk_data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ishUV1aWNML",
        "outputId": "53bed0a0-bfb7-4ce8-a90d-b2bf42ca04bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from attention_layer import AttentionLayer\n",
        "from bilstm_with_attention import BiLSTMAttention\n",
        "from evidence_detection_dataset import EvidenceDetectionDataset\n",
        "from early_stopping import EarlyStopping\n",
        "from vocabulary import Vocabulary\n",
        "from trainer import Trainer"
      ],
      "metadata": {
        "id": "Ju4U0HGxY6TW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embeddings(vocab, embedding_dim=300):\n",
        "    \"\"\"Load GloVe embeddings for words in vocabulary\"\"\"\n",
        "    embeddings = np.zeros((len(vocab), embedding_dim))\n",
        "\n",
        "    # Initialize random embeddings\n",
        "    for i in range(len(vocab)):\n",
        "        embeddings[i] = np.random.normal(scale=0.1, size=(embedding_dim, ))\n",
        "\n",
        "    # Load pretrained embeddings\n",
        "    glove_path = f'glove.6B.{embedding_dim}d.txt'\n",
        "\n",
        "    if not os.path.exists(glove_path):\n",
        "        print(f\"GloVe embeddings not found at {glove_path}. Using random embeddings.\")\n",
        "        return embeddings\n",
        "\n",
        "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
        "\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(tqdm(f, desc=\"Loading GloVe\")):\n",
        "            try:\n",
        "                values = line.split()\n",
        "\n",
        "                # Check if the vector has the correct dimension\n",
        "                if len(values) != embedding_dim + 1:  # +1 for the word itself\n",
        "                    print(f\"Warning: Line {i} has {len(values)} values, expected {embedding_dim + 1}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                word = values[0]\n",
        "                if word in vocab.stoi:\n",
        "                    vector = np.array(values[1:], dtype='float32')\n",
        "\n",
        "                    # Double-check vector dimension\n",
        "                    if len(vector) != embedding_dim:\n",
        "                        print(f\"Warning: Vector for word '{word}' has dimension {len(vector)}, expected {embedding_dim}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    embeddings[vocab.stoi[word]] = vector\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Loaded {embedding_dim}-dimensional GloVe embeddings.\")\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "LKKKt7fcWhJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create output directory for models and plots\n",
        "os.makedirs('models', exist_ok=True)\n",
        "# os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "# Load datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_df = pd.read_csv('./data/train.csv')\n",
        "val_df = pd.read_csv('./data/dev.csv')\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}, Validation shape: {val_df.shape}\")\n",
        "print(f\"Label distribution in train: {train_df['label'].value_counts().to_dict()}\")\n",
        "print(f\"Label distribution in val: {val_df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "# Load or create vocabulary\n",
        "vocab_path = 'vocab.pkl'\n",
        "if os.path.exists(vocab_path):\n",
        "    print(f\"Loading vocabulary from {vocab_path}\")\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "else:\n",
        "    print(\"Creating new vocabulary\")\n",
        "    vocab = Vocabulary(freq_threshold=3)\n",
        "    all_texts = train_df['Claim'].tolist() + train_df['Evidence'].tolist()\n",
        "    vocab.build_vocabulary(all_texts)\n",
        "\n",
        "    # Save vocabulary\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump(vocab, f)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 300\n",
        "embeddings = load_glove_embeddings(vocab, embedding_dim)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EvidenceDetectionDataset(train_df, vocab)\n",
        "val_dataset = EvidenceDetectionDataset(val_df, vocab)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "# Model hyperparameters\n",
        "hidden_dim = 256\n",
        "num_layers = 1\n",
        "dropout = 0.4\n",
        "\n",
        "# Initialize model\n",
        "model = BiLSTMAttention(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout,\n",
        "    pretrained_embeddings=embeddings\n",
        ")\n",
        "\n",
        "# Print model architecture summary\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")\n",
        "print(f\"Embedding Dimension: {embedding_dim}\")\n",
        "print(f\"Hidden Dimension: {hidden_dim}\")\n",
        "print(f\"Number of LSTM Layers: {num_layers}\")\n",
        "print(f\"Dropout Rate: {dropout}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-5\n",
        "num_epochs = 15\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Train model\n",
        "print(\"\\nStarting training...\")\n",
        "trainer.train(num_epochs=num_epochs)\n",
        "\n",
        "# Final evaluation\n",
        "print(\"\\nPerforming final evaluation on validation set...\")\n",
        "val_metrics = trainer.evaluate()\n",
        "\n",
        "print(\"\\nFinal Validation Metrics:\")\n",
        "print(f\"Loss: {val_metrics['loss']:.4f}\")\n",
        "print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
        "print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
        "print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
        "print(f\"F1 Score: {val_metrics['f1']:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = 'models/final_model.pt'\n",
        "trainer.save_model(final_model_path)\n",
        "\n",
        "print(\"\\nTraining and evaluation completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OBaBa6JxJ8Z",
        "outputId": "550c9a6d-8022-4893-986f-ab692fc1134b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading datasets...\n",
            "Train shape: (21508, 3), Validation shape: (5926, 3)\n",
            "Label distribution in train: {0: 15654, 1: 5854}\n",
            "Label distribution in val: {0: 4286, 1: 1640}\n",
            "Loading vocabulary from vocab.pkl\n",
            "Vocabulary size: 14401\n",
            "Loading GloVe embeddings from glove.6B.300d.txt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading GloVe: 400001it [00:07, 54091.14it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 300-dimensional GloVe embeddings.\n",
            "Train dataset size: 21508\n",
            "Validation dataset size: 5926\n",
            "\n",
            "Model Architecture:\n",
            "Vocabulary Size: 14401\n",
            "Embedding Dimension: 300\n",
            "Hidden Dimension: 256\n",
            "Number of LSTM Layers: 1\n",
            "Dropout Rate: 0.4\n",
            "Total Parameters: 5,398,832\n",
            "Trainable Parameters: 5,398,832\n",
            "\n",
            "Starting training...\n",
            "Starting training on device: cuda\n",
            "Training set size: 21508\n",
            "Validation set size: 5926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 673/673 [00:18<00:00, 36.92it/s]\n",
            "Validating: 100%|██████████| 186/186 [00:02<00:00, 71.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - Time: 20.93s\n",
            "Train Loss: 0.4317, Train F1: 0.5666\n",
            "Val Loss: 0.3938, Val F1: 0.6736\n",
            "Val Precision: 0.6572, Val Recall: 0.6909\n",
            "Validation F1 increased (0.000000 --> 0.673603). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 673/673 [00:18<00:00, 36.97it/s]\n",
            "Validating: 100%|██████████| 186/186 [00:02<00:00, 71.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15 - Time: 20.92s\n",
            "Train Loss: 0.3315, Train F1: 0.7163\n",
            "Val Loss: 0.3700, Val F1: 0.6583\n",
            "Val Precision: 0.7170, Val Recall: 0.6085\n",
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 673/673 [00:18<00:00, 37.19it/s]\n",
            "Validating: 100%|██████████| 186/186 [00:02<00:00, 71.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15 - Time: 20.80s\n",
            "Train Loss: 0.2470, Train F1: 0.8079\n",
            "Val Loss: 0.4107, Val F1: 0.6784\n",
            "Val Precision: 0.6757, Val Recall: 0.6811\n",
            "Validation F1 increased (0.673603 --> 0.678409). Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 673/673 [00:18<00:00, 37.18it/s]\n",
            "Validating: 100%|██████████| 186/186 [00:02<00:00, 71.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15 - Time: 20.80s\n",
            "Train Loss: 0.1544, Train F1: 0.8883\n",
            "Val Loss: 0.6256, Val F1: 0.6425\n",
            "Val Precision: 0.6484, Val Recall: 0.6366\n",
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 673/673 [00:18<00:00, 36.84it/s]\n",
            "Validating: 100%|██████████| 186/186 [00:02<00:00, 71.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15 - Time: 20.96s\n",
            "Train Loss: 0.0866, Train F1: 0.9419\n",
            "Val Loss: 0.7397, Val F1: 0.6385\n",
            "Val Precision: 0.6411, Val Recall: 0.6360\n",
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 673/673 [00:18<00:00, 37.06it/s]\n",
            "Validating: 100%|██████████| 186/186 [00:02<00:00, 70.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15 - Time: 20.89s\n",
            "Train Loss: 0.0540, Train F1: 0.9655\n",
            "Val Loss: 0.9310, Val F1: 0.6240\n",
            "Val Precision: 0.6935, Val Recall: 0.5671\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping triggered\n",
            "Loaded best model from 'best_bilstm_attention_model.pt'\n",
            "\n",
            "Performing final evaluation on validation set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 186/186 [00:02<00:00, 71.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Validation Metrics:\n",
            "Loss: 0.4107\n",
            "Accuracy: 0.8213\n",
            "Precision: 0.6757\n",
            "Recall: 0.6811\n",
            "F1 Score: 0.6784\n",
            "Model saved to models/final_model.pt\n",
            "\n",
            "Training and evaluation completed!\n"
          ]
        }
      ]
    }
  ]
}